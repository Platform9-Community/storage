#################################################################################################################
# Define the settings for the rook-ceph cluster with common settings for a production cluster on top of bare metal.

# This example expects three nodes, each with two available disks. Please modify it according to your environment.
# See the documentation for more details on storage settings available.

# For example, to create the cluster:
#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml
#   kubectl create -f cluster-on-local-pvc.yaml
#################################################################################################################
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  placement:
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - "true"
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
#    osd:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: storage-node
#              operator: In
#              values:
#              - "true"
#      tolerations:
#      - key: node-role.kubernetes.io/master
#        operator: Exists
    mgr:
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  cephVersion:
    image: quay.io/ceph/ceph:v16.2.7
    allowUnsupported: false
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mgr:
    count: 1
    modules:
      - name: pg_autoscaler
        enabled: true
  dashboard:
    enabled: false
    ssl: true
  crashCollector:
    disable: false
  storage:
    storageClassDeviceSets:
      - name: set1
        count: 9
        portable: false
        tuneDeviceClass: true
        tuneFastDeviceClass: true
        encrypted: false
        placement:
          nodeAffinity:
            required:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: rook.io/ssd-set1
                      operator: In
                      values:
                        - true
          tolerations:
            - key: node-role.kubernetes.io/master
              operator: Exists
        resources:
          limits:
            cpu: "500m"
            memory: "1Gi"
          requests:
            cpu: "250m"
            memory: "1Gi"
        volumeClaimTemplates:
          - metadata:
              name: data
              annotations:
                crushDeviceClass: ssd-set1
            spec:
              resources:
                requests:
                  storage: 20Gi
              storageClassName: local-storage
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
      - name: set2
        count: 3
        portable: false
        tuneDeviceClass: true
        tuneFastDeviceClass: true
        encrypted: false
        placement:
          nodeAffinity:
            required:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: rook.io/ssd-set2
                      operator: In
                      values:
                        - true
          tolerations:
            - key: node-role.kubernetes.io/master
              operator: Exists
        resources:
        volumeClaimTemplates:
          - metadata:
              name: data
              annotations:
                crushDeviceClass: ssd-set2
            spec:
              resources:
                requests:
                  storage: 60Gi
              storageClassName: local-storage
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
    # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
    onlyApplyOSDPlacement: true
#  resources:
  #  prepareosd:
  #    limits:
  #      cpu: "200m"
  #      memory: "200Mi"
  #   requests:
  #      cpu: "200m"
  #      memory: "200Mi"
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api
